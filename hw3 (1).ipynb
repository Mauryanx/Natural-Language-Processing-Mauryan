{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6RxqIirisTj"
      },
      "source": [
        "# HW3\n",
        "\n",
        "In this homework, we'll learn about transformers and chatbots.\n",
        "\n",
        "It will probably be easiest to run this on http://colab.research.google.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minGPT Character Language Model\n",
        "\n",
        "First, will inspect Karpathy's [minGPT](https://github.com/karpathy/minGPT/tree/master) library to learn more about transformers.\n",
        "\n",
        "We'll first fit a character language model using mingpt. We'll use as training data all the text of Shakespeare."
      ],
      "metadata": {
        "id": "whaHOZpKj_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the library\n",
        "!git clone https://github.com/karpathy/minGPT.git"
      ],
      "metadata": {
        "id": "f_q_lNGEjs7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef89db7-11f4-495d-b50a-d8199e330781"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'minGPT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add mingpt to your Python path, so you can import it.\n",
        "import sys\n",
        "sys.path.insert(0, './minGPT')\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "set_seed(3407)"
      ],
      "metadata": {
        "id": "eDv1T3BNjwzb"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download shakespeare data\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "mPHv0dGquVpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e6fa66-f6db-474a-aa62-2a592c4e6878"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-04 01:42:18--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.4’\n",
            "\n",
            "\rinput.txt.4           0%[                    ]       0  --.-KB/s               \rinput.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-04 01:42:18 (31.5 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and training code"
      ],
      "metadata": {
        "id": "fOnS25h6iSAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
        "import os\n",
        "import sys\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This represents a dataset of characters.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def get_default_config():\n",
        "        C = CN()\n",
        "        C.block_size = 128\n",
        "        return C\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "        self.config = config\n",
        "        self.parse_data(data)\n",
        "\n",
        "    def parse_data(self, data):\n",
        "        print('parsing char data')\n",
        "        # get list of all characters\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        # map from char to int\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        # map from into to char\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.config.block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.config.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # return as tensors\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "def get_config():\n",
        "\n",
        "    C = CN()\n",
        "\n",
        "    # system\n",
        "    C.system = CN()\n",
        "    C.system.seed = 3407\n",
        "    C.system.work_dir = './out'\n",
        "\n",
        "    # data\n",
        "    C.data = CharDataset.get_default_config()\n",
        "\n",
        "    # model\n",
        "    C.model = GPT.get_default_config()\n",
        "    C.model.model_type = 'gpt-micro'\n",
        "\n",
        "    # trainer\n",
        "    C.trainer = Trainer.get_default_config()\n",
        "    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def train_model(config, train_dataset, sample_fn):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    config..........CfgNode\n",
        "    train_dataset...Dataset that emits strings for training\n",
        "    sample_fn.......function to call during training to show sample output.\n",
        "    \"\"\"\n",
        "    # construct the model\n",
        "    config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "    config.model.block_size = train_dataset.get_block_size()\n",
        "    model = GPT(config.model)\n",
        "\n",
        "    # construct the trainer object\n",
        "    trainer = Trainer(config.trainer, model, train_dataset)\n",
        "\n",
        "    # iteration callback\n",
        "    def batch_end_callback(trainer):\n",
        "\n",
        "        if trainer.iter_num % 10 == 0:\n",
        "            print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "        if trainer.iter_num % 500 == 0:\n",
        "            # evaluate both the train and test score\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # sample from the model...\n",
        "                context = list(train_dataset.itos.values())\n",
        "                completion = sample_fn(context, model, trainer, train_dataset, maxlen=100, temperature=1.)\n",
        "                print('sample from the model:')\n",
        "                print(completion)\n",
        "            # save the latest model\n",
        "            print(\"saving model\")\n",
        "            ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            # revert model to training mode\n",
        "            model.train()\n",
        "\n",
        "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "    # run the optimization\n",
        "    trainer.run()\n",
        "    model.eval()\n",
        "    return model, trainer\n",
        "\n",
        "def configure_model(max_iters=100, block_size=128):\n",
        "    config = get_config()\n",
        "    config.merge_from_args(['--trainer.max_iters=%d' % max_iters,\n",
        "                            '--data.block_size=%d' % block_size,\n",
        "                            '--model.block_size=%d' % block_size])\n",
        "    setup_logging(config)\n",
        "    set_seed(config.system.seed)\n",
        "    return config\n",
        "\n",
        "\n",
        "def create_char_data(config):\n",
        "    # construct the training dataset\n",
        "    text = open('input.txt', 'r').read()\n",
        "    return CharDataset(config.data, text)\n",
        "\n",
        "def sample_from_char_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ''.join([train_dataset.itos[int(i)] for i in y])"
      ],
      "metadata": {
        "id": "frsP7S3DuMhT"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the character model.\n",
        "config = configure_model(max_iters=100, block_size=64)\n",
        "train_dataset = create_char_data(config)\n",
        "model, trainer = train_model(config, train_dataset, sample_from_char_model)"
      ],
      "metadata": {
        "id": "hpp2SypliPPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52fb19b-f5a6-4727-8a2c-1daa326c60d0"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 100\n",
            "command line overwriting config attribute data.block_size with 64\n",
            "command line overwriting config attribute model.block_size with 64\n",
            "parsing char data\n",
            "data has 1115394 characters, 65 unique.\n",
            "number of parameters: 0.81M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 4.20564\n",
            "sample from the model:\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzogrku shgfZgh ZfbumiZg bgZorukZomiZhitg bm f bgr s s anekmmymkZmxenenm tmumyre\n",
            " iiZim,hha\n",
            "rkdgbe m\n",
            "g\n",
            "saving model\n",
            "iter_dt 20.61ms; iter 10: train loss 3.22833\n",
            "iter_dt 15.81ms; iter 20: train loss 2.97854\n",
            "iter_dt 18.45ms; iter 30: train loss 2.83429\n",
            "iter_dt 20.84ms; iter 40: train loss 2.72577\n",
            "iter_dt 19.14ms; iter 50: train loss 2.63684\n",
            "iter_dt 22.19ms; iter 60: train loss 2.62312\n",
            "iter_dt 20.17ms; iter 70: train loss 2.55452\n",
            "iter_dt 17.72ms; iter 80: train loss 2.52026\n",
            "iter_dt 17.74ms; iter 90: train loss 2.50115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_from_char_model(\"where are \", model, trainer, train_dataset, maxlen=10, temperature=3))"
      ],
      "metadata": {
        "id": "ty_WfOozuVN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c58baafa-8225-4461-ce5b-fa99083e31f1"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "where are anthith,\n",
            "B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `block_size` variable? Describe in detail what it does.**\n",
        "\n",
        "You might want to consult the code for [model.py](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py).\n",
        "\n"
      ],
      "metadata": {
        "id": "jB30f-tj-uyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The block_size variable sets the length of the sequence that is paid attention to for the output token. SO the model converts the sequence of length that is set by the block_size variable into tokens and these tokens in additon to their positional embeddings are paid attention to when generating the output token.The block_size also helps in the training of the model.with this line of code \"        chunk = self.data[idx:idx + self.config.block_size + 1]\" the model looks at the lenght of input sequence set by the block_size varible takes that as the input and then shifts it by one to look at output sequence which will then therefore include the next word that come after the last word in the input sequenece , it then does that iteratively for the rest of the shakespeare data.The length of the block_size variable can also help determine the dependencies it can capture however a lnger block_size is more computationally expensive\n"
      ],
      "metadata": {
        "id": "jgMZ82gwi1ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the relationship between `block_size` and the total number of parameters in the model?** That is, if we double `block_size`, what happens to the total number of model parameters?"
      ],
      "metadata": {
        "id": "oppLFY7m_yiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The block_size variable sets the length of the sequence that is paid attention to for the output token. The model then converts this sequence into tokens , which will have token and position embeddings. So increasing the block_size will icnrease the number of tokens the model has to pay attention to.\n",
        "\n",
        "\n",
        "However, since the vocab size is the same , the total number of token and position embeddings should remain the same albeit the model might have to pay attention to more of those token to generate the output.If there is any chnages to the parameters it will be due to the transfromer blocks or the attention mechanisms\n",
        " **"
      ],
      "metadata": {
        "id": "zMMlhs6ji9eW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the `n_layer` parameter? Describe in detail what id does. If we double this parameter, what happens to the total number of model parameters?**"
      ],
      "metadata": {
        "id": "6tjo4Tyu_zAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The n_layer parameter affects the number of layers in the model.Each layer contains tranformer blocks , so increses the number of layers increases the number of tranformers. This means that the mnodel will become deepr and the input will have to sequentially go through more layers , this might help the model to learn more patterns in the data, however this is more computationally expensive.\n",
        "\n",
        "In regards to parameters, each transformer block has its own set of parameters.Therefore increasing the number of transformers by increasing the number of layers will lead to an increase in the total number of model parameters."
      ],
      "metadata": {
        "id": "u82DFB4cAhjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does the temperature paramter do?** See the generate method in [model.py](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L283).\n",
        "\n",
        "Try setting temperature to different values. What do you observe about the output?"
      ],
      "metadata": {
        "id": "r8adX_STRbjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at \"logits = logits[:, -1, :] / temperature\" , in the generate()method.Whats happening here is that the probability for the next token prediction based on the input sequence is being scaled by the temperature. The temperature is used to scale the probabilites of the next token. When the temperature is lower than 1 only the tokens with a high probability are chosen , whereas When the temperature is higher than 1, the scaling affects it so that even output token wich didnt have that high of a probability initially could be chosen\n",
        "\n",
        "With the temperature of 0.1 ,  with the input \"where are \" the output is \"where are the the he\" , the word the the is repeated\n",
        "\n",
        "With the temperature of 3 ,  with the input \"where are \" the output is \"where are w omelll w\" , this is less coherent"
      ],
      "metadata": {
        "id": "l_4ffALfRuET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does [line 148](https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L148) in model.py do? How does this relate to the transformer model?**  "
      ],
      "metadata": {
        "id": "IxqGbnN0oXib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line 148 is \"            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\"\n",
        "\n",
        "a block , i.e a tranfromer is created using the config specified , then this is made into a list containing all the blocks from all the layers that are specified in the n_layers parameter.This is then made into a ModuleList object.In realtion the transfomer model ,this line bascially created a sequence of tranfomrer blocks that the input sequence must go through, each of which can capture different aspects and therefore lead to a more complex understanding.\n"
      ],
      "metadata": {
        "id": "gbcrKfRSogw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Model\n",
        "Now, let's fit a word model instead of a character model.\n",
        "\n",
        "Given a string like:\n",
        "\n",
        "> The cow     jumped over the moon. The moon is full tonight!\n",
        "\n",
        "The `WordDataset` class below should create tokens for each space-delimited string:\n",
        "\n",
        "> ['The', 'cow', 'jumped', 'over', 'the', 'moon', '.', 'The', 'moon', 'is', 'full', 'tonight', '!']\n",
        "\n",
        "Note that multiple space characters are treated as one (Hint: `re` may help here.)\n",
        "\n",
        "Using `CharDataset::parse_data` function above as an example, complete the `parse_data` function below to set the `stoi`, `itos`, `vocab_size`, and `data` attributes of the `WordDataset` class."
      ],
      "metadata": {
        "id": "ijvSHpxC9dtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class WordDataset(CharDataset):\n",
        "  def parse_data(self, data):\n",
        "\n",
        "    words = re.split(r'\\s+', data.strip())\n",
        "\n",
        "    unique_words = sorted(set(words))\n",
        "\n",
        "    self.vocab_size = len(unique_words)\n",
        "\n",
        "    self.stoi = { word: i for i, word in enumerate(unique_words) }\n",
        "    self.itos = { i: word for i, word in enumerate(unique_words) }\n",
        "    self.data = words\n",
        "\n",
        "word_config = configure_model(max_iters=200, block_size=4)\n",
        "word_data = WordDataset(word_config.data, 'The cow jumped over the moon. The moon is full tonight!')\n",
        "word_data.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paya9nGgJwif",
        "outputId": "bc8fd407-9962-4b4b-da94-a6ec74f8f992"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 200\n",
            "command line overwriting config attribute data.block_size with 4\n",
            "command line overwriting config attribute model.block_size with 4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'cow',\n",
              " 'jumped',\n",
              " 'over',\n",
              " 'the',\n",
              " 'moon.',\n",
              " 'The',\n",
              " 'moon',\n",
              " 'is',\n",
              " 'full',\n",
              " 'tonight!']"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_from_word_model(context, model, trainer, train_dataset, maxlen=500, temperature=1.):\n",
        "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = model.generate(x, maxlen, temperature=temperature, do_sample=True, top_k=10)[0]\n",
        "    return ' '.join([train_dataset.itos[int(i)] for i in y])\n",
        "\n",
        "word_model, word_trainer = train_model(word_config, word_data, sample_from_word_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7hjlAJtABsI",
        "outputId": "aa44a542-539a-4f61-bfe1-069933f29af6"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.80M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 2.28699\n",
            "sample from the model:\n",
            "The cow full is jumped moon moon. over the tonight! moon. the over the jumped moon. moon. over The over is tonight! The moon. jumped full The the moon. The moon full over cow The the is full moon moon. jumped jumped the moon. the moon. moon. cow jumped cow jumped moon over moon. is The is full moon. moon full The full tonight! the moon moon. over moon is moon The full the moon. over over the jumped full tonight! The moon. The moon moon. The the The tonight! the over the full is moon The moon The moon. over The the the tonight! over the the moon. moon.\n",
            "saving model\n",
            "iter_dt 14.02ms; iter 10: train loss 0.46275\n",
            "iter_dt 14.16ms; iter 20: train loss 0.28833\n",
            "iter_dt 14.97ms; iter 30: train loss 0.19077\n",
            "iter_dt 14.39ms; iter 40: train loss 0.14534\n",
            "iter_dt 15.27ms; iter 50: train loss 0.10207\n",
            "iter_dt 14.16ms; iter 60: train loss 0.10739\n",
            "iter_dt 14.17ms; iter 70: train loss 0.06961\n",
            "iter_dt 15.44ms; iter 80: train loss 0.08814\n",
            "iter_dt 13.99ms; iter 90: train loss 0.07471\n",
            "iter_dt 14.96ms; iter 100: train loss 0.06453\n",
            "iter_dt 15.35ms; iter 110: train loss 0.04373\n",
            "iter_dt 20.96ms; iter 120: train loss 0.05246\n",
            "iter_dt 21.17ms; iter 130: train loss 0.03886\n",
            "iter_dt 20.17ms; iter 140: train loss 0.07034\n",
            "iter_dt 23.74ms; iter 150: train loss 0.03139\n",
            "iter_dt 24.19ms; iter 160: train loss 0.05917\n",
            "iter_dt 29.97ms; iter 170: train loss 0.05654\n",
            "iter_dt 24.68ms; iter 180: train loss 0.04046\n",
            "iter_dt 23.76ms; iter 190: train loss 0.05336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"The\"], word_model, word_trainer, word_data, maxlen=50, temperature=1.)"
      ],
      "metadata": {
        "id": "fZweO3GBIKsM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "baaf8584-89f8-445d-a618-0bd11392bfb9"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The moon is full tonight! jumped over the moon. The moon is full tonight! jumped over the moon. The moon is full tonight! moon. The moon is full tonight! jumped over the moon. The moon is full tonight! moon. The moon is full tonight! tonight! jumped over the moon. The moon'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipedia\n",
        "\n",
        "With our word model, let's now fit a language model on the Wikipedia page for [New Orleans](https://en.wikipedia.org/wiki/New_Orleans)\n",
        "\n",
        "First, we'll install a library to help us fetch the plain text of a wikipedia page."
      ],
      "metadata": {
        "id": "-lYiFOD3MVhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "RnhwHz7-McWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2db27ec-397d-4522-cb58-291d517f5b0b"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "page = wikipedia.page('New Orleans')\n",
        "print(page.content[:100])"
      ],
      "metadata": {
        "id": "A3pclbQqL2zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f9709c-b734-49b9-9f6a-c7eb1eb5756e"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Orleans (commonly known as NOLA or the Big Easy among other nicknames) is a consolidated city-pa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create new variables `word_config`, `word_data`, `word_model`, `word_trainer` that are analogous to the ones used previously. These should fit a model to the `page` text defined in the previous cell.**"
      ],
      "metadata": {
        "id": "6OB56CJpkKKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE\n",
        "word_config = configure_model(max_iters=200, block_size=4)\n",
        "word_config.model.n_embd = 4000\n",
        "word_config.model.n_layer = 10000\n",
        "word_config.model.learning_rate = 1e-1\n",
        "word_data = WordDataset(word_config.data, page.content)\n",
        "word_model, word_trainer = train_model(word_config, word_data, sample_from_word_model)\n"
      ],
      "metadata": {
        "id": "jVcV90oKMjaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ae9d9f-cb51-4378-ba8f-c846aec38135"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command line overwriting config attribute trainer.max_iters with 200\n",
            "command line overwriting config attribute data.block_size with 4\n",
            "command line overwriting config attribute model.block_size with 4\n",
            "number of parameters: 1.48M\n",
            "running on device cuda\n",
            "iter_dt 0.00ms; iter 0: train loss 8.65115\n",
            "sample from the model:\n",
            "\"51%... \"America's \"American \"Beast\" \"Broadway \"Chapel \"Chep\" \"Corps \"Downtown\" \"Fat \"Fifth \"French \"Highway \"Hollywood \"Jazz \"Murder \"New \"North\" \"Red \"South\" \"Spoons\" \"The \"Twelfth \"Vic\" \"Where \"White \"Witch \"ben-yays\"), \"downriver \"downtown\" \"funerals \"generally \"grandfather \"had \"highly \"jazz \"marsh\" \"most \"no \"one-fourth \"per \"plying \"primary \"r\", \"separate \"the \"totally \"upriver \"uptown\" \"wild \"worst-performing\" #21 $1.5 $36 $5.5 $7 & 'n' (\"RTA\"). (\"Voodoo (\"West (0.30 (1,500 (1,590 (1.7% (10 (1060) (11 (12.4 (15 (169 (1810–1892), (1946–1961) (1954). (1961–1970), (1978, (1986–1994) (1994–2002). (2 (2.5–5.1 (2004) (2005 (2009). (21.7 (28.9 (32 (39 (4,800 (40 (440 (450,000 (470 (5,000 (5.0 (52%) (6 (60 (690 (78.4) (8 (9.1 (910 (980 (BESE). (Callender (Citizens (DMA) (December (Dixie, (Exxon, (French (French: (GNO, (Greenwood (HDLC), (June–August) (Köppen: (LOOP), (La (Louis (MSA) (May (NBA). (NFL) (NOMA) (NOPD), (NOPS), (New (OPSB), (RTA) (SPAWAR) (SPR) (TEPPCO (USDA), (VCC). (WCEF). (Zeta (affranchis (after (all (along (although (and (as (behind (called (commonly (ed.). (eds.). (especially (even (gens (he (home (hot (however, (including (known (largely (locally (mostly (northbound (now (of (or (pronounced (served (sinking)\": (southbound (the (when (which (−12 (−14 (−2.4 (−7 + , ...most 0.0% 0.03% 0.1% 0.2% 0.3% 0.4% 0.6% 0.61 1 1, 1,000 1,271,845 1,500 1,800 1.1% 1.26 1.3% 1.7% 1.8 1.9% 10, 10,000 10,000-watt 10.1 100 100% 100,000 100,000, 100-year 100. 102 103 104 105 10th 11 11, 11,000 11th 12 12%, 12, 120,000 12th, 13 13, 13,000 13.5 137,000 13th, 14% 14,000 140 14th 15 15,000, 150,000 151,753 158 159. 15th 16, 169 17 17% 17, 17-year 170,000 1710s 1715 1716 1718 1720s 1723. 1724, 1727. 1729 1730s. 1731, 1740s 1740s, 1747 1748, 176 1760s, 1763 1763, 1765 1768, 1779. 1791 1792. 1796. 18.8% 1800 1800. 1800s, 1800s. 1803, 1803. 1808. 1809 181 1810, 1812, 1814 1815 1815). 1815. 1820 1820-1980 1820. 1827, 1830. 1830s 1830s, 1835 1840 1840, 1840s, 1850s, 1852 1852. 1859. 1860 1860, 1862, 1863 1864 1866 1868 1868, 1868. 1870, 1872 1872, 1873 1874 1874, 1876, 1877, 1889, 1889. 1891 1892 1896. 1899, 18th-century 1900 1902 1909, 1915, 1923, 1929 1938, 1945, 1947 1948. 1950, 1950s 1954, 1955. 1956 1956, 1960 1960, 1960. 1960s 1960s, 1963 1964 1965 1965, 1970 1970s 1970s, 1970s. 1972. 1979 198,000 1980, 1980. 1980s 1980s, 1981) 1981, 1986, 1989 1989). 1989, 1990, 1990s, 1990s. 1991-2020 1991. 1993, 1994 1994, 1995, 1996. 1997, 1998, 1999, 19th 19th-century 1st 1–2 2 2,000 2,731 2.8 2.8% 2.9% 20 20.2% 200 200+ 2000 2000, 2000. 2000s. 2002, 2003-04 2004 2004, 2004. 2005 2005, 2005. 2006 2006, 2006. 2006–2007 2007 2007, 2007. 2007–2008 2008 2008, 2008. 2009 2009, 2009. 200s 2010 2010, 2010. 2011 2011, 2011. 2012 2012, 2012-13 2012. 2013). 2013, 2013. 2014 2014. 2015, 2015-16 2015. 2016 2016, 2016. 2017, 2017. 2018 2018, 2018-19 2018. 2019 2019, 2019–20. 202 2020 2020, 2020. 2021 2021, 2021. 2022 2022, 2024 2025. 20th 21% 21st 22, 223,000; 23, 23.0 230 24 24% 24, 24.5%. 242 25 25) 250 250,000 255,000, 26, 26-year 265 27, 27.6 273,000, 274,000 28 28, 280 29, 3,000 3,000-mile 3,102 3,200-foot 3,226 3.1-mile 3.71% 30 300 300. 31, 31,000 31.61% 310. 32,000 33.0% 336,644 336,644. 33:1500. 345 35% 35,000 350 365 37 38 38,338 383,997 386,617—80% 39 395 4 4.5 4.8% 4.9% 40 40% 409 424 45 450 454,865, 46th 47 49.1 4th-largest 5 5% 5,000 5,000-foot 5.3% 5.7% 50 50%. 50,000 50,000-watt 50,000. 500 51% 510. 510/LA 53 53.2, 53.61% 54.3 54th 55 56% 56.9 566,960 57 57.88 59 6 6,150 60% 60.2% 61 61, 610 62.5 627,525. 63 64 65%—is 659,000 67.4% 68% 69,370 7 7, 7,000 7.3% 70 70% 70.6 70.6. 71.1 73.4 78th 7th 8 8, 8,000 8.08% 8.7 80 80% 80.5 800,000 81 83,722 84 85,000 89,216 9 9.7% 90 90.3% 91% 91. 95% 96 97% 978-0-300-15576-1. 9b, 9th == === ==== A A. AM). ASCE AT&T, About Abraham Academic Academy Acadiana, Accidental According Accounts Act Adams, Additionally, Administration Adrien Advance Advocate Advocate, Advocate. African African, African-American Africans Africans. Afro-Caribbean Afro-Creole After Agency, Agriculture Air Airline, Airport, Airports Alaska Algiers Algiers, Algiers. All All-Star Along Also Although Amendment, America\", America\". America's America, America: American American, American. Americana, Americans Americans, Americans. Americas Among Amtrak. An Andrew Angeles. Anglo-Americans Anne Another Antebellum Anti-Italian Anti-Spanish Appalachian Appeals Approximately April Aquarium Archdiocese Archive Archives Archon Area. Arena ArenaBowl, Arizona Armed Armstrong Armstrong, Army Arrested Art Art. Arts As Ash Ashkenazi Asian Assembly Association At Athletic Atlanta Atlantic Atlas. Audubon August August. Aurelia. Aurelian, Aurelianum. Authority Avenue Avenue, Avenue. Avenue/Canal B. BCS BNSF BP, Back Bad Baldwin Ballet Baltimore) Baltimore. Bank\" Bank, Bankston Baptist Baptist, Barataria Barthelemy Base Baton Battle Battlefield Bayou Beatles Because Bee), Before Beginning Belle Belt Belt's Benjamin Berkeley Berkeley, Bernard Bernard, Bernardo Berry, BestPlaces. Beth Betsy Better Between Bible Bicycling Bienville, Big Billboard Biographical Birdman, Biscuits, Bisso Black Blake Blaze, Blond Board Board, Boasso Bob Bobby Boeing. Boggs Bollinger Bonapartist Book Borgne Boston Botanical Both Bouligny Bourbon Bourgeois Bowl Bowl, Bowyer Brazilian Brazilian, Brazilians Bridge, Bridgeline, Bridges Brief, Brigadier Brinckerhoff, Bring Britain British British, Broadmoor. Broutin, Brown Built Bulls Bureau Bureau, Buses Business But Butler Butler, Button, By Bywater CH2M COVID-19 CSX Cab Cab's Caesars Café Cajun Cajun, Caldas, California California, California: Calumet, Cambridge Cambridge, Cameron Canada, Canadian Canal Canal, Cantrell Capital Capital\", Capital, Capital: Car Care Caribbean Caribbean, Carl Carnival Carnival, Carolina Carolina's Carolina), Carre Carrollton Carrollton, Carré Carré), Case Cash Catastrophe, Cathedral, Catholic Catholic. Catholicism Causeway, Cemetery Cemetery, Census Center Center, Center-Hall Center. Central Centuries Cfa), Chalmette Chalmette, Chalmette. Championship Channel, Chapel Charles Charles, Charleston, Charter Chasse Chasse. Chemical Chernobyl\", Chevron, Chevron. Chicago Chickasaw Chicken Chinese, Chitimacha. Choctaw Chouest Christ Christian Christians Christmas Christmas. Chronicle Church Church, Cindy Circle, Circuit Cities\" Citoyens City City, City: City; Cityscape Civil Claiborne Claiborne. Clarion Class Classic Classic, Claude Clay Climate Close Club: Co., Coast Code Coffee Cohen Collection Collection, Collective College Colleges Collins); Colonial Colonial, Colonies Colored Comité Command, Commission Commission, Committee) Commonly Community Companies Company Company, Concerns Confederacy Confederate Conference Conference. Congo Congregation Congress Connection Conoco, Considering Constitution Consultants Contemporary Control Convent. Convention Convention, Corp, Corps Cottages Council, Count Courrier Course, Court Courthouse Covington, Cowboy Creole Creole, Creoles Crescent Crescent, Crime Criminal Crow Crow. Crowbar, Crown Crude Cuba Cuban Cuban, Cuisine Cultural Culture Cups, Curious D.C., Dallas, Daniel Dash Data DeLesseps Decadence, December Deep Delta Delta, Democratic Democrats, Demographics Department Dept. Desire Desire, Desire. Despite Dessens, Detroit. Development Developments Dewpoints Dialect Dictionary Disaster District District\" District) District, District. Districts Division Dixie Dixiecrats Dominican Douglas Dow Down Downtown Drainage Dreams: Duke Dupuy During Dutch Dwarfing Dylan Dynegy, Déjà Each Earl Early East East. Eastern Easy Easy, Easy: Economic Economy Edison Education Edward Election. Elementary Elevation Elsewhere Elysian Emancipation Emergency Empire Energy Energy, Engineering Engineers England, English English-only English. Englishes, Eni Entergy, Enterprises, Entertainment Entertainment, Environmental Epiphany, Episcopal Ermus, Esplanade Esplanade, Essence Estimates Etymology Europe European European, European-style Eve Every Evidence Examples Exchange, Expansion Expansion. Expeditors Experience Experience, Exploration Explorer, Expressway. External Eyehategod, Ezra, F. FMP, Facility Fair Faubourg Favorite Feast February Federal Federal, Ferguson, Ferries Ferry Ferry) Ferry, Fertel, Fest\") Fest\", Fest, Festival Festival, Festival. Field) Fields Fifth Filipino Filipino, Filipinos Film Films Final Finance Flood Floodwalls Florida Florida, Florida: Flossy Folgers, Following Fontainebleau For Force Forgot, Fort Forts Fortune Forum Forwarding Foundation. Founded Fountain Four. Fourchon, Fourteenth France France's France, Francisco), Francophone Francophones Francophonie Frantz François Freeport-McMoRan, French French, French-language French-speakers French-speaking French. French–Spanish Freret From Frontier: Further G., GE Gainesville, Gallic Gallup Gambit Game Game, Garden Garden. Gardens Gary, Gas Gates Gen. General Gentilly, Geography Geology Georges Georgia Georgia's Georgia. German German, Germans, Germany, Gert Ghent, Girls Girls: Gitlin, Given Global, Glory God Gold Gospel Government Governor Grand Gras Gras, Gras. Great Greater Greek Green Green, Greenway Gretna, Grier Griffin Griffin's Grounds Growing Grumman Gulf Gusman Gustav Gustav. Gálvez HDLC Hackberry Haitian Hale Hall Hall. Harrah's Harvard He Henry Herald, Heritage Highway Hill, His Hispanic Historic Historical Historically, History Holiday, Holli, Homer Honduran, Honey Hornets Hot Hotel House House. Housing Houston, Howard Howard-Tilton However, Huey Hurricane Hurricane, Hurricanes Hutson, Hydrologic I I-10 I-10's I-10, I-610 IBM, II II, II. IMTT, ISBN Iberville Ida Ida, If Ignace Ildefonso Imagining Imperial Improvising In Inability Inc. Inc.) Inc., Indeed, Indian Indian, Indiana, Indies, Industries, Influenced Information Insectarium), Inspector Institute Insurance, Integrated Intermarine, International International, Internationale Interstate Intracoastal Irish Irish, Iron Isaac Islamic Island Islander, Isle Isleño Israel's Israel, It Italian Italian, Italianate Italians Italians, Italians. Its J. J., Jackets. Jackson Jackson, Jamaica Janet January January, Jason. Jay Jazz Jean Jean-Baptiste Jefferson Jefferson, Jewish Jews Jews, Jews. Jim John, Joint Jones, Journal Julia July June Jury, Juvenile, K. KIPP, Kansas Katrina Katrina's Katrina), Katrina, Katrina. Kazmi. Kenner, Kenner. Kentucky Kinder King King's King. Kingdom Kingdom, Koch Koch, Korean), L'Abeille L. LA: LGBT La LaKisha LaToya Labatt Labor Lachaise Lachance, Lafayette Lafayette, Lafitte Lafitte), Laitram, Lake Lake, Lakefront Lakefront, Lakeview Lakeview, Lakeview. Land Landmark Landmarks Large Last Later Later, Latin Latin, Latino Lauderdale Launch Laveau Law Lawrence Le Leadership League League\", Led Lee Leer Leisure Lent, Less Levees Liberty Libraries Library Library, Library. Life Like Lil Limit Limited's Limited, Lincoln. Line Liquefied Literary Lives Local Locap); Lockheed Long Longue Los Louis Louisiana Louisiana's Louisiana) Louisiana, Louisiana-Canarian Louisiana. Louisiana: Louisianan Love\" Lower Loyola Loyola-UPT Ltd, Luther Lyft, MSA MWH Machine Madrid, Magazine Major Management Many Marathon Marc March Mardi Marie Marigny Marigny, Marine Marine, Market Market. Marler, Marlin Marquis Martin Martin, Marvin Mary Mass. Massachusetts Massachusetts: Master May Mayors, McAlister McDermott McMoRan Media Medical Melvin Memorial Memphis Men, Merchants' Metairie, Metairie. Methodist Metropolitan Mexican, Mexico Mexico, Mexico. Miami Michelle. Michoud Mid-City Mid-City, Mid-Valley, Middle Migration Military Militia, Million Milnor, Mine Minerals Minnesota. Mint Mint, Mint. Missionary Mississippi Mississippi, Mobile, Modal Monday Monde, Money Monroe More Morgan Morial Morrison Morrison's Most Mostly Mountains. Mouth Moyne Much Murguía, Museum Museum, Museum—a Music Muslims N. NASA NASA's NBA NC: NCAA NOLA NOLA, NOLA.com. NOPD NOPS Nagin Named Napoleon Natchez Natchez, Nathalie. National Native Native, Nativist Nature Naval Navtech, Navy Navy's Navy, Nearby, Nearly Nefutzot Neighborhood Nelson Network, Network. New Newpark Nicaraguan. Night\" Nineteenth-Century Ninth No No. Noir, Norfolk North Northeast, Northeastern Northrop Notable Notably, Notes Nous Nouvelle Nouvelle-Orléans Nouvelle-Orléans), Nova November Nueva October Of Office Official Officially, Offshore Offshore, Ogden Oil Old On One One, Oneida One–University Only Opera Opera. Operated Order Organisation Orlando. Orleanians Orleans Orleans\", Orleans\". Orleans' Orleans) Orleans, Orleans,) Orleans-based Orleans. Orleans; Orleans—no Orléans Orléans, Orléans. Orthodox Other Outlet Over P, P. P.B.S. PGA Pacific Pakenham, Pan Panic Panthers, Paris Paris, Paris. Parish Parish. Park Park), Park, Park. Parsons Partners Partners, Party Party, Party. Passenger Pauger, Paul Pelican Pelicans Pelicans) Pelicans, Pellerin People Perhaps Perier Peter Petroleum Pew Philadelphia, Philip Philip, Philippe Phoenix. Picayune Pierre Pierre, Pinchback, Pitt Place, Places, Plan, Plantation, Plaquemines Plaza Plessy Plessy, Po' Point Poles Police Political Politics Pontchartrain Pontchartrain, Pontchartrain. Pool Popeyes Popp Port Portugal Portuguese Post-disaster Postal Powell, Power. Poydras Prayer Pre-Katrina, Precipitation Preservation Preserve Preserve. President Press Press, Press. Primary Prior Prison. Privateers Proclamation Product Project Protestant Public Publications Puerto Purchase Purchase, Père Quarter Quarter\" Quarter) Quarter, Quarter. Quarter—is Queen R. R.R. RTA RTA's Race Radiators, Rail Railroad Railroad, Railway, Railway. Raised Rampart Rampart–St. Ray Ray, Raymond Real Rebecca Rebecca, Rebuilding Receivables Reconstruction Reconstruction. Records Records. Recovery Redeemers, References Reform Refuge Regents Regiment Regional Register Relations Relatively Religion Religious Republic. Republican Research Reserve Reserves Resilience. Resources, Revised Revisited\". Revival, Revolt Revolution, Revolutionary Rican, Rien. Rights Riot Riots Rip Rise Risk, Rita Rita's River River, River-Gulf River. Riverfront River–Gulf Road Road, Roads Robert Rock Rock. Roll Rollergirls, Rolls-Royce, Roman Rosalie. Rouge Rouge, Rouge. Roughly Royal Ruby Rue Rugby Runaway Running S. Saint Saint-Domingue] Saints Saints, San Saturn Sauvage Savannah, Schiro Schiro's, School Schools Scott Scurloch-Permian, Seaplane Secondary Sector\". See Seed Segregated Separate Sephardic Sephardim, September Service Service, Service. Services, Services. Serving Seven Several Sexual Shell Shell, Sheriff's Ship Shipping, Shipyards, Shuttles, Sicilian), Sidney Significantly, Silocaf. Simmons, Since Sir Sister Slave, Smoothie Snedeker, Snow Snowstorm Society Soilent Solnit, Some Soon, South South\" South\". South, South. South.\" South: South; Southern Southland Space Spain's Spain). Spain, Spain. Span Spanish Spanish) Spanish, Spanish. Specifically, Sperling's Sports Square Square, Square. St. Stadium, State States States, States-Item States. States; Statewide Station Station, Statute Stephen Steve Stewart Storage Strategic Street Street\", Street\". Street, Street. Street/Gretna Streetcar Streetcars Striffler Structure Subsidence, Sucré Sugar Suicideboys, Sully. Sun Sunset Super Superdome Superdome. Superior Supreme Survey, Susan Swamp Syed System. Systems Systems, TEPPCO, TP Tallest Tammany Taxi Tech Tech's Technology Tejas, Television Tennessee Tens Terminal Territory Terrytown Texaco, Texas Texas. Textron Than That The Theatre Their Then-Mayor There Thereafter, These They Third Thirteen This Thomas Though Thousands Threat Through Throughout Thus, Théâtre Tidewater Times-Picayune Times-Picayune. Times-Picayune/The Today Today, Together, Toups' Toups, Tour Tour. Tourism Tourist Towboat, Tower Town, Towns, Trade Traders, Trading, Trail, Trail. Transit Transit, Transoceanic Transportation Travel Treaty Tremé Tremé, Trinity Troops, Truss Tuesday Tuesday\"), Tulane TurboSquid, Twenty Twin Two Two-thirds U.S. U.S.' U.S., UNO US. USDA USL UTP). Uber Uber's Ullman Unable Unfathomable UnidosUS, Unified Union United University University's University, Unlike Unocal, Until Up Upper Uptown Uptown. Uptown; Ursuline V Van Vaudreuil, Vaudreuil. Victor Victorian Vietnam Vietnamese Vietnamese, Vieux Violence Virginia Visitors Volunteers VooDoo, Voodoo Voodoo, Voodoo. Voting Vu, Vue WCEF WNOE WTIX WTUL WWII WWOZ, Waldemar War War). War, War. Ward Ward), Ward, Ward. Warehouse Warfare Warmouth Wars Wartime War–Reconstruction Washington, Waterway/Mississippi Wave Wayback Wayne, Website Wednesday. Weekly Weekly. West When While White White, Whitney Wilder, Wildlife William Williams Williams/New Witchcraft, With Within Women Wood's Wood, Working World World. World: XLIV XLVII. XV XV's Xavier Yachts, Yaka Yale Yat Year Year's Years Years' Yehudah Yellow York York, York: Young Zatarain's, Zeta Zoo, Zurich Zydeco [from [that] [ˌnweβa a able abolished about above above-ground absolute abundance abuses academic academy. accent accents accents. access accessible accommodations accompanies according accounts achieved achievement, acquired acquitted acronym across acted action active actively activities. activity actors. acts actual added adding addition addition, additional addressing adherents. adjacent, adjusted administered administration administration, administrations administrative adopted advancement advances advantage adversely advertisements affairs affected affiliate affiliates affluent after after, aftermath aftermath, afternoon again against age agencies agencies. aggravated aid airport airports albeit aligned all all, all-female all-white alleged alleviate allies. allocated allowed allowing alluding almost alone, along alongside already also although always ambassadors ambitious ameliorated amended. amendment among amount amounting an analysis ancestors anchors ancillary and and, anglicized annexed anniversary, annual annually annually, annually; another antebellum antique any appeared apply appointed approach approached approaches appropriates approximately architect architects architectural architecture architecture. archives are are, are: area area's area, area. area; areas areas, areas. armies around arrested. arrival arrived arrived. arriving art article, artifacts artisan, artists. arts as as-needed aspects, assaulted assessment assessors assessors, assigned associate associated assumed at at-large, athletic attainment attainment, attempt attempted attend attended attending attracted attractions attractions, attractive au authentic authorities authority auto/bicycle/pedestrian automation automobiles. autonomously average averaged averaging avocation avoided awarded aware back back. backup balconies, band, bands bands, bank banker banks baptized barrels barrier bars bars, base baseball based basement basis. bayous. be beans became because become becoming bed beef been before began began. beginning begins behind beignets beignets) being beliefs beliefs. believed below beneath beneficiary. benefit benefits best between bicycle bicycle. bicycles bicycles. bicycling bicycling. bicyclists big biggest bike biked biking billion billions biodiesel. biographies birth bisexual, black black, blacks blacks. blacks; blamed bleed blend blending bluegrass blues blues, blues. boarded boarding boasts bodies boiled bolster bolstering books. boom booming born both bottom bounce bounce, boundaries bounded boutique bowl boy branch branches brand brass break breakfast brew bridge bridges bridges; brief briefly bring bringing broadcasts broader broadsheet broken brokerage brought brown budget budget. build building buildings buildings, buildings. built bulk bungalow bus buses busiest business businesses but butter, buying by bypass cabs, cabs. café cafés/coffee cajun, cakes call called calliope came campaign campaign, campus, can can't canceled. candidate candy cannot capable capacity capacity. capita capital capture captured car car, care career, carefree cargo carpooled, carries carrying cars case catastrophic catastrophically categories, category catering cause, caused causeway causing ceased ceded celebrated celebrations celebrations. cemeteries cemeteries. cemetery census census, census. censuses center center. central centralized cents. centuries. century century, century-long century. certain cessation challenges, champion change changed changes chapter characteristics characterized charter charters checkpoints cheese, chicory chief chief. child children children. choose chooses church; churches, circulation cities cities), cities, cities. citizens citizens. citizenship city city's city, city-parish city-parish. city. city; civil claims claims: class classes classical classical, clause\" clay. clean cleanliness, climate climbed close-knit closed clothing, clustered cm). coast coast, coast. coastal cobbled cocktail coffee coffee); coffee-roasting cold coldest collection collections. collectively college colleges collegiate colonial colonial-era colonists colonists, colonization colony colony, colony. color color. combination combine combined commanders commences commentators commerce. commercial commercially commissioned committee commodities commodity communication communities communities, communities. communities: community community's community, community. commute commuted commuter commuters commuters. companies company company, company: comparable compared compares competed competing competition competitive completely complex complex, composed concentrated concentration concern condensed conditions conducive conflict, conflicts confrontations congregation connect connecting connection connects consecrated consecutive consensus consider considered consistent consistently consisting consists consolidated consolidation consortium constituted constitution constitutional constitutional, constitutional. constructed constructing construction containerization contains contemporary contiguous continent. contingent continually continue continued continuing continuous continuously contribute contributed contributing control controversy controversy. convening convent convention conventions converting cooperate coordinating copper corporate correctional corresponding corridor. corridors cost coterminous. cottages cotton could couleur council count counter counterbalanced counting countries, country country's country, country. course court court-ordered courts: courtyards cousin coven coven, cover cowpunk, crafts, crawfish cream, create created creating creation creative credited credits creole creolophone crime crime, crimes criminal crop crops cross-cultural crossed crossing crossings crossings, crucial crude cruises cuisine cuisine, cuisine. cuisines. culminating cultivated cultural culturally culture culture. cultures cultures. current currently curve. cut cycles cycling. cyclones d'A., d'Afrique\". d'Orléans d'Orléans) daily daily. damage damage, damage. damaged dangerous date dates day day. days days-long days. de deaths debate. decades decay. decennial decisively declared decline decline. declined declined. decorated decrease decreased dedicate defeat defeated defined degree degree-granting degrees deletion deliver delivery delivery). demanding demands. demographic demographic. demonstrated densely departed departing department dependent depopulation. deported deposition depot derby derives des descent descent), descent, descent. described describes desegregation, design designated designed destination destination. destinations destinations, destinations. destroy destroyed destroyed, destruction detained. determined devastated develop developed developing development development, devised devoted dialect dialect, dialects, did died different direct directed direction directly dirges disaster disastrous disc diseases. disfranchised dishes; dislocation dispersed displaced displacement display displayed displays disproportionately disputed disrupt disruption disruptions distillation distinct distinctive distinctly distinguishing distributed distribution district districts districts. diverse diversification diverting divides dividing division do does doesn't dollars dollars, dollars. domestic dominant doom double-gallery double-track doubled doubling doubt dough doughnuts\" down downtown downtown. dozens drain drainage drainage, draining dramatically drew driest drilling driving dropped dropped, dropping dry. du dubbed due during dying each earlier earliest early ease easily east east, eastbank eastern east–west easygoing, eclipsed economic economy economy. economy—transportation, edge edition edition, editions eds. educated educated, education education. educational effectively effects effort efforts eighth either elderly elderly, elected election elections electronic elegant elevating elevation elevation, eleven eliminate elite emperor employed employment. enabled enacted end ended ending ends endured energy enforcement\" engage engineer engineer-in-chief engineering engineers engines, enormous enormous, enough enroll enrolled enslaved enslaved, entire entirety entrance entrepreneurial entrepôt environment. epidemics epochal equal\" equality equally, equipment equivalent era era, eroding erosion escaped. especially established established, establishing establishment estimate estimate, estimated estimates etc., ethnic ethnicity evacuated. evacuation even event, events events, events. eventually every ex-slaves example example, exceed exceeded excepting exception exclusively executive exempted existing exit expand expanded expanded. expanding expansion. expansive experienced experiences experiencing exploration export expressed extend extended extending extension extension, extensive extensive, external extreme faced facilitate facilities facilities, facilities. facilities: fact fact, factors factory faded. failed failed, failure faith, fame famed families family famous fan far far-reaching fare farthest fast favorite fear feared, feasibility feature featured features federal federally fees, feet fell female ferry festival festival, festivals festivals, festivals. festivities, fever few fewer fifth fifth-highest fifth-largest fifty fighting figures, filled film films final financial find finding fire firms firms, first first-place five five-mile flat flatboats flatware. flavor. fled fleet flights flights. flood flood. flooded flooded. flooding flooding, flooding. floodproofing floodwalls floor flooring, focused\" focusing folk, followed following food food. foot football footprint for for. for: force forced forces foreign form formal formed formed, former formerly forms fortifications, forum, forwarding fought found foundations founded four four-year fourteen frame, francophone free free, freedmen freezing freight frequencies frequent frequently fresh fried friendliness from front. frontiersmen fuel full full-blood fullback funds funerals funerals\". funerals, funerals. funk further future gaining gains game games gap gardens gas gateway gather gatherings. gave gay gay, gay-friendliness gender general generally generated generation generations. gens gentrification, geographic gifts given gives giving globally globe going gold golf gone good goods goods, gospel, goth, governing government government's government, government. governmental governments governor governor, governor. gradually grandest granted granulated great greater greatly green greeting grew grid ground ground. groundwater group group, groups growing growing, grown growth guarding gubernatorial guides, gumbo gun guns had half half-shell, hand, handle handled handling happened, happier harassed hard-to-find hardcore hardcore, hardiness has haute have having hazardous he head-to-head headlines headquartered headquarters heady health heard heart heavy held help helped helping hence heritage heritage. high high, high. higher highest highly highs, highway highways him himself hint hip his historian historic historically history history, history. hit hitting holding hollows. home home, home. homes. homicide homicides. honor hop hop, hoping hospitality host hosted hostilities hosts hot, hotbed hotels hotels, hotels/inns, hours hours, house household household. households households, houses houses, housing how however, hub hub. huge human human-induced, humid hurricane hurricane, hurricane-induced hurricanes hurricanes, hurricanes. hymns) iSeatz, icy. ideas identified identity if ignore ignored, immediately immensely immigrant immigrants immigrants, immigrants. immigrated immigration impact impacts implemented implying importance important imported imports, impose imposed impoverished improvement in in, inaugural incentives incentivize inches incidence incident, incited include include: included included. includes including income incorporated incorporating increase increased increasing increasingly incumbent independence independent independently index indicated indie indigenous individual industries industry industry, industry. infectious inflation) influenced influences influences. influential influential. influxes infrastructure. ingredients, inhabitants inhabitants. inhabited inspired inspiring installations installed instance instead instituted institution institutionalizing institutions institutions. instruction instruments insurgent insurgents intact integrated integration. intense interchange intercourse, interest interests interference. interior international internationally, intersection interstates, intertwined into intricate inventor investments involving iron is is, islands), isolation issue issued issued. it it. its itself jail jambalaya, jazz) jazz, jazz. job jobs jobs. jockeys join judge judgements judgment judgments judgments, juries jurisdiction just justice keep key kids' killed killed, king km) km2) km2), knocked knowledge-based known la labor laborers laborers. lagged lait land landfall landmark. landmarked lands landscape, lanes language language, language. languages, large large, largely larger larger. largest largest, last late late-20th later latter latter's launch launched law laws laws, lawsuit. lawsuits, lawyer layers leaders leading league league. learned learning, least leaving led left legal legislature legislature. lending length lesbian, less lesser letters, levee levees levees, level level, level,\" level. levels levels. level—and liberating libraries library libres), lie lies lieutenant life like likely limited limits line line, line. lineage lines: links list list. listed listener-supported liturgical live lived lives local local, localized locally locals located locations. locomotives logistics, long long, longer longest longtime looting loses loss lost low low- low-lying lower lowest-performing lows loyal loyalty. lunch lying lynching lynchings m) m3/d), made magnitude mail mail. main mainly mainstay maintain maintained maintains major majority make makeup making male male; males mall man-made managed mandatory mansions mansions) manual manufacturing manufacturing, manumissions manumitted many marched maritime mark marked market market. markets. married marsh marshland marshlands mass massive matched matter may mayor mayor's mayor-council mayoral mayors mean means measured measures measures. measures—such median medical meet meetings mein. melting members members, memorabilia. memorial men mental merchant merchants merged met metal metal, metric). metric, metropolitan mid-19th mid-20th mid-range mid-rise middle middle-class midsection.\" migrated migration mild miles military militia militia, million ministered ministers minor minority minority, minutes, minutes. misportrayed missions mixed mixed-race mixture mm) mob mobs modern moment money money. month month. monthly months more more, morning most most-visited mostly motels mother motorcycle, moved movement movement's movement. much muffuletta mulatto, multi-building multicultural multilingual multiple multiracial murder murdered. murders murders. murders: museum museum, museums music music\". music, music. music/concerts music: musical musically musician musicians nadir name name, named namely names, namesake narrow nation nation's nation, national nationwide native natural naturally nature navigating near nearby nearby, nearest nearly necessarily need needed negative negligible neighborhood neighborhood. neighborhoods neighborhoods. neighboring neither network never new newer newly news newspaper newspaper, newspapers newsstand next nickname nicknamed nicknames nicknames) nicknames, night nightclubs nightclubs. nightlife nights nine nineteenth no nominal non-Catholic non-Hispanic non-commercial. non-denominationals, nonstops nor normals norms north north, northeastern northern not notable notable. notably noted noted, now nuclear number numbers numerous oak observed observers occasions. occultist occupation occupied occupying occurred occurred\". occurred. occurrence occurrences occurs odyssey of off off-limits off-shore offenders offer offering offers office office, office. officeholders. officer officers officers. offices offices, offices. official officially officials offshore often oil oilfield old older older, oldest on on-demand. once one one-way one-year one—the ongoing online online; only only, only. onshore onward open open, opened opera opera, operate operated operated, operates operating operation operations operators opportunities opposition or order ordinance ordinary organic organizations organized orientation origin original originally originated other others others) otherwise out outbreak outlying outside outside. outwardly over overall overrunning overtopping overwhelmingly owing own own, owner owners. oxidation oysters oysters, oɾleˈans]), p. paid paid. paper papers paradigm parallel paramilitary parents parish parishes parishes. parks, parlance parlance). parochial paroisse part particular particularly partnership party party. pass passage passed passenger passengers passing passions past past, path pay pay. pay.\" peace peak peak. pecans. pedestrian pedestrians pedestrians. peer people people, people. per percent percentage perfectly,\" performance performance. performances. performed performing perhaps period period, period. permanent permanently persisted person, personal persons petrochemical petroleum physical physically pilfered pipelines: pirate place placed places plague plan planes planned planned. plans plans, plant plantations. play play, played plays playwright po' point point-of-contact point. points police policeman policy policy-makers political politics poll poll, poor poorer poorly pop populace popular popularity populated population population's population, population. population; populations. populous port port, portion portions portions. ports pose possesses possession possible possibly post-Katrina) post-industrial, posted posted. postponed, pot potentially poverty poverty, power powered practice, practitioner praline pre-Katrina pre-Katrina, pre-consonantal preceded precipitation predominance predominantly preferred prejudices presence presence. present preservation. president pressing pressure previous previously price primarily primary prime print printed printing prior priority private privateers problem problem. problems processions produce produced producers producing production production, production. productions professional professor profitable. program. programming programming. programs programs, prohibits prominent promote promoting prompted pronounced proper proper's proper. properly propertied property proper—about proportion proportionately prosperity prosperous prostitutes. protect protected protection protection. protections provide provides providing province provision provisions proximity public public. publication publications publicly published pump pumping pumping. pumps punk, purchased put quantity quarters, quicker quite race race, race. races races. racial racially radical radio raid raids rail railroads railroads. railways rain, rain-induced rainfall, raised rally ran range ranges ranging ranked rankings ranks rapid rapid, rapidly rare rare, rate rate\" rates rates. rather ratified re-established re-institute re-opened re-routing reach reached reaches reaching reading readmitted reaffirmed reasonably rebellion rebound rebuilding received receiving recent recently recently, recession recognizable recognized recommended record recorded records records, recovery recovery. recruit recruited red redefined redevelopment reduce reduced reemerged refer reference referred referring refineries, refining reflect reflected reflooded refuge refugee refugees refuses regained regaining regent reggae, regiments region region's region, region. region; regional regions regulars, rejected related relating relation relations relative relatively released religion religious relinquished relocate, relocated relocations remain remained remains remains, renamed renewed renovation renowned renters, repair repeated replacement replenishing repopulation report representatives. representing represents request requesting required rescued research researchers resembles reserved reserves. resettled resided residential residents residents, residents. residing resign. resort respected respectively. respectively; rest restaurants restaurants, restore restored restoring restrictions result result, resulted resulting results resumed resumption retain retained retaliation retaliation, returned returned. revealed revenue revenues revenues. revered reverted review revised revolutionaries, reworked rhythm rhythms. rice ricely ride: ridership, ridges right rights, rights. rigs. rioting risen rises. rising risk risk. risks rivals. river river's river, road rock rock, rocket role roll. roller rooms rooms. rose roster, route routes rule rule. ruled ruling running runs runway rural sad safety said sail sailing sale sales. same sand, sandwich. sandwiches; sat saw says scale scenes, schedule scheduled schemes. scholarly school schools schools, schools. scientific scientists score scores sea seafood; seaplanes. season season, season. seasons second second-largest secondary sections sector sector, sector. security, sediment sediments see seen segregated segregation seized seizure sent sentiment separate separately. series serious serve served serves service service, service. services serving set settle settled settlement settlement. settlers, settling seven seventeen several severe severe, severely shares she shelters sheriff sheriff's shifted shipbuilding, shipment shipping, ships ships. shop shops shops, shops. shore short short, shortcut shot shotgun should showcasing showed shows, showtunes, shrank shrinking shut shutting siege signed significance, significant significantly silt, silver similar simple simply since singers single single-member singles/bar sink. sinking sisters sit-ins site situation situation, six six-year-old sixth sizable size, ska, skilled skills skills. skyline skyrocketed, skyscrapers' slave slavery slaves slaves. sleet, slightly slipping slowed slower slowly sludge small small, smaller smuggling snow snowfall snowstorm so so, so-called so. soccer society socioeconomic soft software soils sold solely some sometimes somewhat song song, sought soul/funk, sound sources, soured south south, southeast southeastern southern southward sovereignty spaces, span) span), span. spared spawned speakers speakers, speakers. special specialist. specialized specializes specialties specialty specific specifications spending, spiked spin spoke spoken sponsored sporting sports spot spread spring spur spurred square square-shaped stable stadium staff, stage, standardization. stands staple start started started. state state's state, stated stated: states station. stations statistical statistics stayed steadily steam steamboat steamboats, stereotypical still storage stores stores. storm storm, stranglehold strategic strategically strategies street streetcar streetcars streets streets\", streets, streets. strengthened stretches strike strong strongest strongly struck), structures structures. struggled struggles student students students). students. studies.\" studio study sturdy sturdy—cannot style. styles styles, stylish sub-Saharan subdivision subsequent subsequently subsidence subsidence, subsidence. subsiding substantial subtropical suburb suburban suburbanization suburbs suburbs. succeeded success successes successful successfully such suffered suffrage sugar sugar, sugarcane suggested suggests summarize summer summers; supplanted supplemented supplies support supported supportive supports supposedly suppress suppressed suppressing surface, surfaced. surge surge. surges, surges. surpassed surpassing surrounded surrounding survey surviving susceptible sustained swamp swamp's swamp. swampy sweet swept swing synagogue synagogues syncretism system system's system). system, system. systems tabloid tackle tactics take taking tallest tankers. tanks tax taxi taxicab, team, team. teams techno, technology telegram television temperature temperatures temporarily temporary ten ten-day tenants tens term terminal terminates terminus terms territorial territory's territory. test than than, that the theater theaters. theatre their them them, them. themselves, then then, then-governor there there, there. these these: they third third-most third-oldest thirteenth thirty-eight thirty-minute this this, thoroughbred those though thousand thousands threat threatened three through throughout tile time time, time. times title titles to today today. together tolled tombs tonnage tonnage. tons took top top-50 topic tornado total total, tourism tourism, tourism. tourist tournament toward townhouses, trace track track. tracks tracts trade trade, trade. traded traders trades trading tradition. traditional traditions traditions, traditions. traffic traffic, traffic. trail train trained transatlantic transferred transgender transit translate transportation transportation, transporting travel travels traverses treat treating treaty treaty). trees trends tribes tribes, tried trips, troops tropical truly turned turns twelfth-largest twelfth-most twice two two- two-fourths two-thirds ultimately unaccounted uncommon under underfunded underlie understaffed understand undertaken undocumented undocumented. unintended unions unique unique\" uniquely unit universal universities universities, university, unknown) unless unpaid unrest until up upgrade upholding uplift upriver upstream uptick. uptown upward upward, urban urbanized usage use used uses usually utility v. value value. valued variable variant varieties variety various vast vehicles, venues very vessels via viability. vibrant victims viewed vigorous vintage violence, violent violently virtually visible visibly, visited visitor visitors voices volume, volume. volunteered volunteers, voodoo vote vote, voters voting vulnerability. vulnerable walk walked walked. walking wanted war, war. ward. wards. warehoused warfare warned wars was watching. water water. waterborne watershed watershed. wave, way weakness weakness. wealth, wealthier wealthiest website website, week week, weekends weekends\", weekly well well), well-educated, well. went were were, west west. westbank wetland wetlands wettest, what wheeled when where whether which which, while white white, white-collar white. whites whites, who whom whose wide widely widely. widespread winning winter winter, winters with with, withdrew within without women women's words, work worked workers working works world world's world, world-famous world-renowned world. worst would wrote y y'at?\" year year, years years. yellow yet young yours\"). zenith zone zydeco, °C) °C)+ °C), °C). °F Étienne étouffée, – football Philippe pecans. season. of vulnerable achievement, Academy pecans. accents. scores accents. of Perier dry. of of Georgia. times of easily of New Georgia. water vulnerable Academy known achievement, times Relations football degree-granting accents. achievement, achievement, dry. Academy strike Philippe of times Epiphany, Academy 2012. Clay Georgia. inspiring marshland Wednesday. Company accents. English-only degree-granting rooms rooms Academy inspiring known accents. achievement, achievement, heady season. vulnerable Clay inspiring seizure accents. English-only east aftermath, Epiphany, of accents. football Philippe inspiring Relations Epiphany, incident, incident, achievement, water Company of Company breakfast accents. of Epiphany, degree-granting mulatto, pecans. accents. of New of times water\n",
            "saving model\n",
            "iter_dt 14.57ms; iter 10: train loss 8.11172\n",
            "iter_dt 14.58ms; iter 20: train loss 7.83252\n",
            "iter_dt 15.24ms; iter 30: train loss 7.55956\n",
            "iter_dt 14.24ms; iter 40: train loss 7.29415\n",
            "iter_dt 14.63ms; iter 50: train loss 7.17514\n",
            "iter_dt 22.33ms; iter 60: train loss 6.86374\n",
            "iter_dt 23.08ms; iter 70: train loss 7.07304\n",
            "iter_dt 16.46ms; iter 80: train loss 6.93585\n",
            "iter_dt 21.30ms; iter 90: train loss 6.87021\n",
            "iter_dt 29.22ms; iter 100: train loss 6.61334\n",
            "iter_dt 45.93ms; iter 110: train loss 6.68409\n",
            "iter_dt 44.92ms; iter 120: train loss 6.58379\n",
            "iter_dt 28.73ms; iter 130: train loss 6.82150\n",
            "iter_dt 25.22ms; iter 140: train loss 6.37607\n",
            "iter_dt 59.34ms; iter 150: train loss 6.67578\n",
            "iter_dt 44.35ms; iter 160: train loss 6.45372\n",
            "iter_dt 40.31ms; iter 170: train loss 6.21375\n",
            "iter_dt 34.10ms; iter 180: train loss 6.47514\n",
            "iter_dt 32.89ms; iter 190: train loss 6.38227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_from_word_model([\"A\", \"local\", \"variant\", \"for\", \"hip\", \"hop\", \"is\"], word_model, word_trainer, word_data, maxlen=200, temperature=1.)"
      ],
      "metadata": {
        "id": "cg1v3nK2QKi-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1e209d21-3789-43ad-dec8-48fcb71c0c9d"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A local variant for hip hop is by the city in New Orleans The Mississippi Orleans in the city's city and its the United of other of the city and a complex and the city was the New Orleans in the city and the city in the French Mississippi Orleans was the city's city to the Mississippi Orleans and New Orleans. The New Orleans and the French city and New Orleans in the New Orleans and the city to the city The city and Orleans in the city in the city to the city is its a and the city to the Mississippi Orleans and the city in the Louisiana In New Orleans to the United American city and the city and the French New Orleans a and American city's city and the city to the New in New Orleans was the New Orleans is the city and New Orleans is The city's Louisiana and the U.S. Mississippi Orleans and the city and the U.S. city's city is The New as the U.S. Mississippi Orleans' and the city to the city of other in the U.S. New Orleans was the French city to the United city's city and the Louisiana and African New Orleans in the New\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investigate different model settings (`block_size, max_iters, learning_rate, n_embd, n_layer`).\n",
        "\n",
        "**What effect do you notice from trying different values? Which setting appears to generate the best generated text?**\n"
      ],
      "metadata": {
        "id": "LjKzZ1gHglDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing block_size:changing the block size from 1,4,10,12 did not show that much dfference , the text was still not coherent.\n",
        "Changing max_iters :changing it from 10,100,200,100  showed some difference 10 was not coherent at all it become better as iterations increases but between 200 and 1000 there was not that much difference.\n",
        "Changing learning_rate :changing it from 1e-10,1e-7,1e-3,1e-1  did not show that much of difference.\n",
        "Changing n_embd :changing it from 1000,4000,7000,10000 did not show that much of difference.\n",
        "hanging n_layer :changing it from 1000,5000,8000,10000  also did not show that much of difference.\n"
      ],
      "metadata": {
        "id": "mHxKPzVag3E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you wanted to take the word model trained on the New Orleans Wikipedia page and use supervised fine-tuning to create a chatbot that answers questions about New Orleans.\n",
        "\n",
        "**What type of additional training data would you need to do this?**\n",
        "\n",
        "Provide example data below."
      ],
      "metadata": {
        "id": "jkacGUNjVwhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Wikipedia is a good source for General and historical information. It would be good to have some practical infomration from locals , using blogs like these https://uniquenola.com/blog/tour/being-a-new-orleans-local-walking-tour/\n",
        "\n",
        "Also having data in a question-answer pair would be ideal for training a chatbot\n",
        "\n"
      ],
      "metadata": {
        "id": "QPCc4ZlngSTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If this new data contains words that don't appear in the New Orleans wikipedia page, what will happen? How can you fix this?**"
      ],
      "metadata": {
        "id": "qEM97S3xgTjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we get out of vocabulary words , this might be an issue.The model would be as cofident and therefore this might affect the output generated. One way to fix this this might be to when getting OOV words to expand the vocabulary using the fine-tuned data"
      ],
      "metadata": {
        "id": "ZcO_yJkxgdzf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}